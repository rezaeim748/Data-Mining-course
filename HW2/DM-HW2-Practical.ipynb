{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e875a6-0490-4389-829f-a8bfd0979711",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=center style=\"line-height:300%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "سری دوم تمارین عملی درس داده کاوی\n",
    "</font>\n",
    "</h1>\n",
    "<h3 align=center style=\"line-height:100%;font-family:vazir;color:#0099cc\">\n",
    "استاد درس: دکتر مریم امیر مزلقانی\n",
    "</h3>\n",
    "<h5 align=center style=\"font-size: 20px;line-height:100%;font-family:vazir;color:#0099cc\">\n",
    "طراح تمرین عملی: محمد چوپان  \n",
    "    </h5>\n",
    "<p align=center style=\"font-size: 16;line-height:100%;font-family:vazir;color:#0099cc\">\n",
    "    <a href=\"mailto:Aut.DataMining.Fall@gmail.com\">Aut.DataMining.Fall@gmail.com</a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c200f-a461-453a-ba3a-9ea0b558ad95",
   "metadata": {},
   "source": [
    "<div dir=\"ltr\">\n",
    "<p style=\"font-size: 16;line-height:100%;font-family:vazir;color:red;dir:rtl\">\n",
    "لطفا خواسته های تمرین را در بخش خودش انجام دهید.\n",
    "دقت کنید نتایج مقایسه ها حتما باید در پاسخ شما وجود داشته باشد.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d8c93-0f04-4968-80cb-f7f14dd3f9b4",
   "metadata": {},
   "source": [
    "# HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3183f-34af-4051-97f6-ce78134bf973",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "811e5873-e438-4f82-96ae-3bf21c9236ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac2907-3789-45a1-841d-a3f543edaef6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c327de4-e924-4cca-a1aa-ef0db30cf5ff",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله همانند تمرین قبلی داده ها را درون یک دیتا فریم pandas ذخیره میکنید\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c104a97-be13-4918-95be-43f57de59d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ville ville_ascii      lat       lng         pays iso2 iso3  \\\n",
      "0  A Coruña    A Coruna  43.3667   -8.3833        Spain   ES  ESP   \n",
      "1  A Yun Pa    A Yun Pa  13.3939  108.4408      Vietnam   VN  VNM   \n",
      "2  Aabenraa    Aabenraa  55.0444    9.4181      Denmark   DK  DNK   \n",
      "3    Aachen      Aachen  50.7756    6.0836      Germany   DE  DEU   \n",
      "4    Aadorf      Aadorf  47.4939    8.8975  Switzerland   CH  CHE   \n",
      "\n",
      "                admin_nom capital  population            id  \n",
      "0                 Galicia   minor    245468.0  1.724417e+09  \n",
      "1                 Gia Lai   minor     53720.0  1.704946e+09  \n",
      "2              Syddanmark   minor     16401.0  1.208000e+09  \n",
      "3  North Rhine-Westphalia   minor    249070.0  1.276806e+09  \n",
      "4                 Thurgau     NaN      9036.0  1.756023e+09  \n"
     ]
    }
   ],
   "source": [
    "## TODO\n",
    "# load data\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "file_path = \"worldcities.xlsx\"\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the data was loaded\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993415e6-2590-4314-b4f7-0e60b8938171",
   "metadata": {},
   "source": [
    " ## Add column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf5cfc-6c85-45af-abfd-3b15417d2b9f",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این مرحله با توجه به ستون جمعیت یک ستون جدید تحت عنوان Population Level   اضافه کنید به طوری که به 4 دسته زیر  بر اساس وضعیت هر داده اختصاص داده می شودند.\n",
    "</br>\n",
    "    <div dir=\"ltr\">\n",
    "Low: 0-25%\n",
    "    </br>\n",
    "Mid: 25-50%\n",
    "    </br>\n",
    "High: 50-70%\n",
    "    </br>\n",
    "Over: 70-100%\n",
    "    </div>\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1072ca6b-2077-4922-bc17-50d98f1a6563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ville ville_ascii      lat       lng         pays iso2 iso3  \\\n",
      "0  A Coruña    A Coruna  43.3667   -8.3833        Spain   ES  ESP   \n",
      "1  A Yun Pa    A Yun Pa  13.3939  108.4408      Vietnam   VN  VNM   \n",
      "2  Aabenraa    Aabenraa  55.0444    9.4181      Denmark   DK  DNK   \n",
      "3    Aachen      Aachen  50.7756    6.0836      Germany   DE  DEU   \n",
      "4    Aadorf      Aadorf  47.4939    8.8975  Switzerland   CH  CHE   \n",
      "\n",
      "                admin_nom capital  population            id population_level  \n",
      "0                 Galicia   minor    245468.0  1.724417e+09             Over  \n",
      "1                 Gia Lai   minor     53720.0  1.704946e+09             Over  \n",
      "2              Syddanmark   minor     16401.0  1.208000e+09              Mid  \n",
      "3  North Rhine-Westphalia   minor    249070.0  1.276806e+09             Over  \n",
      "4                 Thurgau     NaN      9036.0  1.756023e+09              Low  \n"
     ]
    }
   ],
   "source": [
    "## TODO\n",
    "# Calculate the quartiles to determine population levels\n",
    "low_threshold = df['population'].quantile(0.25)\n",
    "mid_threshold = df['population'].quantile(0.50)\n",
    "high_threshold = df['population'].quantile(0.75)\n",
    "\n",
    "# Function to categorize population into levels\n",
    "def categorize_population_level(population):\n",
    "    if population <= low_threshold:\n",
    "        return 'Low'\n",
    "    elif population <= mid_threshold:\n",
    "        return 'Mid'\n",
    "    elif population <= high_threshold:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Over'\n",
    "\n",
    "# Apply the function to create the 'population level' column\n",
    "df['population_level'] = df['population'].apply(categorize_population_level)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2da931-7aef-4ec8-9631-a7f84928a572",
   "metadata": {},
   "source": [
    "## Preproccess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896cb24-ac84-4bf7-986f-44e8dd46da7c",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    در این بخش باید ستون های خالی داده ها راحذف کرده و در نهایت تمامی ستون ها را به داده های عددی تبدیل کنید به جز ستون اضافه شده\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "926e1b30-615a-4e8a-a9ee-27b5c91ff627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ville  ville_ascii      lat       lng  pays  iso2  iso3  admin_nom  \\\n",
      "0      0            0  43.3667   -8.3833   200    62    63       1088   \n",
      "1      1            1  13.3939  108.4408   235   226   228       1121   \n",
      "2      2            2  55.0444    9.4181    55    54    57       3332   \n",
      "3      3            3  50.7756    6.0836    77    52    54       2433   \n",
      "4      4            4  47.4939    8.8975   207    38    36       3440   \n",
      "\n",
      "   capital  population            id population_level  \n",
      "0        1    245468.0  1.724417e+09             Over  \n",
      "1        1     53720.0  1.704946e+09             Over  \n",
      "2        1     16401.0  1.208000e+09              Mid  \n",
      "3        1    249070.0  1.276806e+09             Over  \n",
      "4        3      9036.0  1.756023e+09              Low  \n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Drop empty columns\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Create a copy of the DataFrame to perform label encoding\n",
    "df_numerical = df.copy()\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Iterate through columns and apply label encoding to non-numerical columns\n",
    "for col in df_numerical.columns:\n",
    "    if col != 'population_level' and not pd.api.types.is_numeric_dtype(df_numerical[col]):\n",
    "        df_numerical[col] = label_encoder.fit_transform(df_numerical[col])\n",
    "\n",
    "# Display the updated DataFrame with numerical values\n",
    "print(df_numerical.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29659ac3-64d9-49be-b3cc-3161735608d2",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    " حال باید داده های به دست آمده از قسمت قبلی را نرمال سازی کنید.\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81cf9e8d-690c-40e0-bd4b-7332c00fea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ville  ville_ascii       lat       lng      pays      iso2    iso3  \\\n",
      "0  0.000000     0.000000  0.719357  0.476966  0.829876  0.259414  0.2625   \n",
      "1  0.000024     0.000024  0.500016  0.802408  0.975104  0.945607  0.9500   \n",
      "2  0.000048     0.000049  0.804814  0.526556  0.228216  0.225941  0.2375   \n",
      "3  0.000073     0.000073  0.773575  0.517267  0.319502  0.217573  0.2250   \n",
      "4  0.000097     0.000097  0.749559  0.525106  0.858921  0.158996  0.1500   \n",
      "\n",
      "   admin_nom   capital  population        id population_level  \n",
      "0   0.270714  0.333333    0.006506  0.773829             Over  \n",
      "1   0.278925  0.333333    0.001424  0.752914             Over  \n",
      "2   0.829062  0.333333    0.000435  0.219122              Mid  \n",
      "3   0.605374  0.333333    0.006601  0.293029             Over  \n",
      "4   0.855934  1.000000    0.000239  0.807778              Low  \n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Select numerical columns for normalization (excluding 'population_level')\n",
    "numerical_cols = [col for col in df_numerical.columns if col != 'population_level']\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize numerical columns using Min-Max scaling\n",
    "df_normalized = df_numerical.copy()  # Make a copy of the DataFrame\n",
    "df_normalized[numerical_cols] = scaler.fit_transform(df_normalized[numerical_cols])\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "print(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e35d0-8209-45c5-9d34-6156d33010d9",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b892a-d9c5-4bdf-93b1-04f93d09479c",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "داده ها را به دو بخش تست و آموزش با نسبت 80 به 20 تقسیم کنید.\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f662ca8-ed93-4d18-a7f4-f887f9bdbbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape - Features: (35733, 11)  Target: (35733,)\n",
      "Test set shape - Features: (8934, 11)  Target: (8934,)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Define features (excluding 'population_level') and target ('population_level')\n",
    "features = [col for col in df_normalized.columns if col != 'population_level']\n",
    "target = 'population_level'\n",
    "\n",
    "# Splitting the data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_normalized[features], df_normalized[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the train and test sets\n",
    "print(\"Train set shape - Features:\", X_train.shape, \" Target:\", y_train.shape)\n",
    "print(\"Test set shape - Features:\", X_test.shape, \" Target:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134641a2-081d-44f1-adc0-95f0b8f1c38b",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf9c80-7547-4c95-84e7-a9093a8ff6ba",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "برای این قسمت باید ستون Population  را به عنوان برچسب درنظر گرفته و از بقیه ستون هابه عنوان فیچر استفاده کنید\n",
    "</font\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "باید از رگرسیون خطی\n",
    "،درجه 1 و درجه 2 استفاده کرده و در هر مورد دقت و خطای میانگین مربعات را بدست آورده و نتایج را باهم مقایسه کنید.</font\n",
    "</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d86ab34a-c2a0-4de9-953e-875a6c5a8e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Linear Regression): 0.00046620911587450154\n",
      "R-squared (Linear Regression): 0.020381865340039607\n",
      "Mean Squared Error (1-degree Polynomial Regression): 0.00046620911587450116\n",
      "R-squared (1-degree Polynomial Regression): 0.020381865340040495\n",
      "Mean Squared Error (2-degree Polynomial Regression): 0.0004551287612263818\n",
      "R-squared (2-degree Polynomial Regression): 0.043664370941418396\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population' and col != 'population_level']\n",
    "target = 'population'\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized[features]\n",
    "y = df_normalized[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='mean')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred_linear = linear_model.predict(X_test_imputed)\n",
    "mse_linear = mean_squared_error(y_test_imputed, y_pred_linear)\n",
    "r2_linear = r2_score(y_test_imputed, y_pred_linear)\n",
    "\n",
    "# Polynomial Regression (degree 1)\n",
    "poly_1_transformer = PolynomialFeatures(degree=1)\n",
    "X_train_poly_1 = poly_1_transformer.fit_transform(X_train_imputed)\n",
    "X_test_poly_1 = poly_1_transformer.transform(X_test_imputed)\n",
    "poly_1_model = LinearRegression()\n",
    "poly_1_model.fit(X_train_poly_1, y_train_imputed)\n",
    "y_pred_poly_1 = poly_1_model.predict(X_test_poly_1)\n",
    "mse_poly_1 = mean_squared_error(y_test_imputed, y_pred_poly_1)\n",
    "r2_poly_1 = r2_score(y_test_imputed, y_pred_poly_1)\n",
    "\n",
    "# Polynomial Regression (degree 2)\n",
    "poly_2_transformer = PolynomialFeatures(degree=2)\n",
    "X_train_poly_2 = poly_2_transformer.fit_transform(X_train_imputed)\n",
    "X_test_poly_2 = poly_2_transformer.transform(X_test_imputed)\n",
    "poly_2_model = LinearRegression()\n",
    "poly_2_model.fit(X_train_poly_2, y_train_imputed)\n",
    "y_pred_poly_2 = poly_2_model.predict(X_test_poly_2)\n",
    "mse_poly_2 = mean_squared_error(y_test_imputed, y_pred_poly_2)\n",
    "r2_poly_2 = r2_score(y_test_imputed, y_pred_poly_2)\n",
    "\n",
    "# Print MSE and R-squared for each model\n",
    "print(\"Mean Squared Error (Linear Regression):\", mse_linear)\n",
    "print(\"R-squared (Linear Regression):\", r2_linear)\n",
    "print(\"Mean Squared Error (1-degree Polynomial Regression):\", mse_poly_1)\n",
    "print(\"R-squared (1-degree Polynomial Regression):\", r2_poly_1)\n",
    "print(\"Mean Squared Error (2-degree Polynomial Regression):\", mse_poly_2)\n",
    "print(\"R-squared (2-degree Polynomial Regression):\", r2_poly_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab615d-f77d-4e93-bbd9-a7c58845be79",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc1ba8-be9d-4a68-8911-f8cb4369e321",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "برای این قسمت باید ستون Population Level  را به عنوان برچسب درنظر گرفته و از بقیه ستون هابه عنوان فیچر استفاده کنید\n",
    "همچنین ستون Population را نیز حذف کنید\n",
    "    </font>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "در هر فسمت باید الگوریتم مشخص شده را پیاده سازی کرده و درنهایت نتیجه را با بقیه قسمت ها مقایسه کنید.\n",
    "    \n",
    "</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aab964-d818-41c2-9085-e3dc6fb736a9",
   "metadata": {},
   "source": [
    "### Decision Tree (Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3d67df5-dcb3-4384-a826-37d521e45fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37541974479516454\n",
      "Mean Squared Error: 2.092119991045444\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "clf.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = clf.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e36a4-db5f-4575-b4bf-adbe415a3608",
   "metadata": {},
   "source": [
    "### Random Forest (Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a300801-83f0-4a99-8f8f-5912f5915379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42287888963510184\n",
      "Mean Squared Error: 1.9582493843743005\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = RandomForestClassifier(criterion='entropy', random_state=42)\n",
    "clf.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = clf.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e9942-7d04-4851-aa31-42926bc4a2f2",
   "metadata": {},
   "source": [
    "### KNN K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "295dd113-5946-42a5-a97f-d730c58f1f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3511305126483098\n",
      "Mean Squared Error: 2.2385269755988357\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "clf.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = clf.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad964b-2f5c-45d0-b2aa-c4c785d5e711",
   "metadata": {},
   "source": [
    "### KNN K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "180e81db-71ed-4e75-8b5f-41da93d5498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3700470114170584\n",
      "Mean Squared Error: 2.2165883143049028\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = clf.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75073d5c-6704-4b5c-93e8-8c72e4ef69f2",
   "metadata": {},
   "source": [
    "### KNN K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5c76909-ab69-4b03-8919-b372dace8feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38191179762704275\n",
      "Mean Squared Error: 2.0457801656592793\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = clf.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531fe89-c5b3-4fe5-8efa-a7f141f8e61e",
   "metadata": {},
   "source": [
    "### SVM (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeb52dae-48cb-4785-beaf-c8a6069986c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3360197000223864\n",
      "Mean Squared Error: 1.8065815983881799\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = svm.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d0a22-f302-497b-9c84-ef028e0bb9b2",
   "metadata": {},
   "source": [
    "### SVM (Non-linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "decea248-30f7-4fbe-8119-5da191d51f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3774345198119543\n",
      "Mean Squared Error: 2.028542646071189\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# TODO\n",
    "# Assuming df contains your dataset and has missing values\n",
    "# Define features and target variable\n",
    "features = [col for col in df_normalized.columns if col != 'population_level' and col != 'population']\n",
    "target = 'population_level_encoded'\n",
    "\n",
    "# Dropping the 'population' column as it won't be used for classification\n",
    "df_normalized_drop = df_normalized.drop('population', axis=1)\n",
    "# Label encode the 'population_level' column\n",
    "label_encoder = LabelEncoder()\n",
    "df_normalized_drop['population_level_encoded'] = label_encoder.fit_transform(df_normalized_drop['population_level'])\n",
    "\n",
    "# Now drop the original 'population_level' column\n",
    "df_normalized_drop = df_normalized_drop.drop('population_level', axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = df_normalized_drop[features]\n",
    "y = df_normalized_drop[target]\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in features using mean imputation for both train and test sets\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Impute missing values in target variable (if any)\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Decision Tree Classifier\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(X_train_imputed, y_train_imputed)\n",
    "y_pred = svm.predict(X_test_imputed)\n",
    "\n",
    "# Calculate accuracy and mean squared error\n",
    "accuracy = accuracy_score(y_test_imputed, y_pred)\n",
    "mse = mean_squared_error(y_test_imputed, y_pred)\n",
    "\n",
    "# Print accuracy and mean squared error\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
